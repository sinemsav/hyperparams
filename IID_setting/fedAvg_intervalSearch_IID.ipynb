{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-publicity",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "DEVICE = 'GPU'\n",
    "numWorkers = len(tf.config.list_physical_devices('GPU'))\n",
    "if numWorkers == 0:\n",
    "    DEVICE = 'CPU'\n",
    "    numWorkers = !cat /proc/cpuinfo | grep processor | wc -l\n",
    "    numWorkers = int(numWorkers[0])\n",
    "    \n",
    "print(numWorkers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-horror",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import talos\n",
    "import os\n",
    "import math\n",
    "from matplotlib import pyplot\n",
    "import tensorflow_datasets as tfds\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from dataset_loader import load_tf_dataset\n",
    "from models_keras import get_model\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-boards",
   "metadata": {},
   "outputs": [],
   "source": [
    "#global parameters for grid search over activation function\n",
    "degree = 3\n",
    "interval = 0\n",
    "\n",
    "def reluApprox(x):\n",
    "    if degree == 3:  \n",
    "        if interval == 3:\n",
    "            return 0.7146 + 1.5000*K.pow(x/interval,1)+0.8793*K.pow(x/interval,2)\n",
    "        if interval == 5:\n",
    "            return 0.7865 + 2.5000*K.pow(x/interval,1)+1.88*K.pow(x/interval,2)\n",
    "        if interval == 7:\n",
    "            return 0.9003 + 3.5000*K.pow(x/interval,1)+2.9013*K.pow(x/interval,2)\n",
    "        if interval == 10:\n",
    "            return 1.1155 + 5*K.pow(x/interval,1)+4.4003*K.pow(x/interval,2)\n",
    "        if interval == 12:\n",
    "            return 1.2751 + 6*K.pow(x/interval,1)+5.3803*K.pow(x/interval,2)\n",
    "    if degree == 5:  \n",
    "        if interval == 7:\n",
    "            return 0.7521 + 3.5000*K.pow(x/interval,1)+4.3825*K.pow(x/interval,2)-1.7281*K.pow(x/interval,4)\n",
    "        if interval == 20:\n",
    "            return 1.3127 + 10*K.pow(x/interval,1)+15.7631*K.pow(x/interval,2)-7.6296*K.pow(x/interval,4)\n",
    "def sigmoidApprox(x):\n",
    "    if degree == 3:  \n",
    "        if interval == 3:\n",
    "            return 0.5 + 0.6997*K.pow(x/interval,1)-0.2649*K.pow(x/interval,3)\n",
    "        if interval == 5:\n",
    "            return 0.5 + 0.9917*K.pow(x/interval,1)-0.5592*K.pow(x/interval,3)\n",
    "        if interval == 7:\n",
    "            return 0.5 + 1.1511*K.pow(x/interval,1)-0.7517*K.pow(x/interval,3)\n",
    "        if interval == 8:\n",
    "            return 0.5 + 1.2010*K.pow(x/interval,1)-0.8156*K.pow(x/interval,2)\n",
    "        if interval == 12:\n",
    "            return 0.5 + 1.2384*K.pow(x/interval,1)-0.8647*K.pow(x/interval,2)\n",
    "def tanApprox(x):\n",
    "    if degree == 3: \n",
    "        if interval == 1:\n",
    "            return 0.9797*K.pow(x/interval,1)-0.2268*K.pow(x/interval,3)        \n",
    "        if interval == 2:\n",
    "            return 1.7329*K.pow(x/interval,1)-0.8454*K.pow(x/interval,3)\n",
    "        if interval == 3:\n",
    "            return 2.1673*K.pow(x/interval,1)-1.3358*K.pow(x/interval,3)\n",
    "        if interval == 5:\n",
    "            return 2.5338*K.pow(x/interval,1)-1.8051*K.pow(x/interval,3)\n",
    "        if interval == 7:\n",
    "            return 2.6629*K.pow(x/interval,1) -1.9801*K.pow(x/interval,3)\n",
    "        if interval == 12:\n",
    "            return 2.7599*K.pow(x/interval,1)-2.1140*K.pow(x/interval,2)\n",
    "    if degree == 12:\n",
    "        print('ooopssss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-danger",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def randomSplitClientsData(data, labels, ds_info):\n",
    "    \n",
    "    numParties = ds_info['num_clients']\n",
    "    sample_height, sample_width, sample_channels = ds_info['sample_shape']\n",
    "    num_classes = ds_info['num_classes']\n",
    "    \n",
    "    numSamplesPerClient = int(data.shape[0]/numParties)\n",
    "    #print(numSamplesPerClient)\n",
    "    clientsData = np.zeros((numParties,int(numSamplesPerClient),sample_height,sample_width,sample_channels))\n",
    "    clientsDataLabels = np.zeros((numParties,int(numSamplesPerClient),num_classes))\n",
    "    #print(numSamplesPerClient)\n",
    "    ind = 0\n",
    "    for i in range(numParties):\n",
    "        clientsData[i] = data[ind:ind+numSamplesPerClient]\n",
    "        clientsDataLabels[i]=labels[ind:ind+numSamplesPerClient]\n",
    "        ind = ind+numSamplesPerClient\n",
    "    return clientsData, clientsDataLabels\n",
    "\n",
    "def prepare_data_for_X_clients(x_train, y_train, ds_info):\n",
    "    clientsData, clientsDataLabels = randomSplitClientsData(x_train, y_train, ds_info)\n",
    "    return clientsData, clientsDataLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-transition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(history, params):\n",
    "    print('##########################################################')\n",
    "    print(params)\n",
    "    # plot loss\n",
    "    pyplot.subplot(211)\n",
    "    pyplot.title('MSE')\n",
    "    pyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "    pyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    "    # plot accuracy\n",
    "    pyplot.subplot(212)\n",
    "    pyplot.title('Classification Accuracy')\n",
    "    pyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
    "    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
    "    # save plot to file\n",
    "    #filename = sys.argv[0].split('/')[-1]\n",
    "    #pyplot.savefig(filename + '_plot.png')\n",
    "    #pyplot.close()\n",
    "    pyplot.show()\n",
    "    print('##########################################################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-professional",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(x_train, y_train, x_val, y_val, params):\n",
    "    \n",
    "    optimizer = SGD(learning_rate=params['learn_rate'], momentum=params['momentum'], nesterov=False, name='SGD')\n",
    "    \n",
    "    model = get_model(params, ds_info)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=\"mean_squared_error\",\n",
    "                  metrics=['accuracy', tf.keras.metrics.Recall(),tf.keras.metrics.Precision()])\n",
    "\n",
    "    history = model.fit(x=x_train,\n",
    "                    y=y_train,\n",
    "                    epochs=params['epochs'],\n",
    "                    batch_size=params['batch_size'],\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=0)\n",
    "\n",
    "    hist.append(history)\n",
    "    hist_params.append(params)\n",
    "        \n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_gridsearch(work):\n",
    "    \n",
    "    client_number, clientData, clientDataLabels, param_grid = work\n",
    "    \n",
    "    free = np.where(workers == 1)\n",
    "    i = free[0][0]\n",
    "    workers[i] = 0\n",
    "\n",
    "    #Distribute load accross DEVICEs\n",
    "    with tf.device(f\"/{DEVICE}:{i}\"):\n",
    "        print(f\"training on {DEVICE}: {i}, interval is {interval}\")\n",
    "        try:\n",
    "            scan_results = talos.Scan(x=clientData,\n",
    "                                      y=clientDataLabels,\n",
    "                                      params=param_grid,\n",
    "                                      model=experiment,\n",
    "                                      experiment_name=f\"{experiment_name}_{client_number}\")\n",
    "            scan_res[client_number]=scan_results\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        print(f\"client running on {DEVICE}: {i} finished\")\n",
    "\n",
    "        workers[i] = 1\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "def grid_search_for_X_clients(num_clients, clientsData, clientsDataLabels, param_grid):\n",
    "    \n",
    "    global scan_res\n",
    "    global workers\n",
    "    global hist\n",
    "    global hist_params\n",
    "    \n",
    "    scan_res = np.zeros(num_clients, dtype=object)\n",
    "    workers = np.ones(numWorkers)\n",
    "    \n",
    "    hist = []\n",
    "    hist_params = []\n",
    "                \n",
    "    work = [(i, clientsData[i], clientsDataLabels[i], deepcopy(param_grid)) for i in range(num_clients)]\n",
    "    \n",
    "    with ThreadPool(len(workers)) as p:\n",
    "        p.map(client_gridsearch, work)\n",
    "\n",
    "    return scan_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-knight",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def client_test_metrics(work):\n",
    "    \n",
    "        client_number, clientData, clientDataLabels, avg_test_params = work\n",
    "    \n",
    "        free = np.where(workers == 1)\n",
    "        i = free[0][0]\n",
    "        workers[i] = 0\n",
    "    \n",
    "        #Distribute load accross DEVICEs\n",
    "        with tf.device(f\"/{DEVICE}:{i%numWorkers}\"):\n",
    "            \n",
    "            print(f\"training on {DEVICE}: {i}, interval is {interval}\")\n",
    "            model = get_model(avg_test_params, ds_info)\n",
    "            optimizer = SGD(learning_rate=avg_test_params['learn_rate'], momentum=avg_test_params['momentum'], nesterov=False, name='SGD')\n",
    "            model.compile(optimizer=optimizer, loss=\"mean_squared_error\",\n",
    "                          metrics=['accuracy',tf.keras.metrics.Recall(),tf.keras.metrics.Precision()],\n",
    "                          run_eagerly=False)\n",
    "\n",
    "            try:\n",
    "                model.fit(x=clientData,\n",
    "                            y=clientDataLabels,\n",
    "                            epochs=avg_test_params['epochs'],\n",
    "                            batch_size=avg_test_params['batch_size'],\n",
    "                            verbose=0)\n",
    "\n",
    "                print(f\"client running on {DEVICE}: {i} finished\")\n",
    "                metrics = model.evaluate(x_test, y_test)\n",
    "                metrics_res[client_number] = metrics\n",
    "                \n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            workers[i] = 1\n",
    "            \n",
    "        return\n",
    "\n",
    "def test_metrics_for_X_clients(num_clients, clientsData, clientsDataLabels, avg_test_params):\n",
    "\n",
    "    global metrics_res\n",
    "    global workers\n",
    "    \n",
    "    metrics_res = np.zeros(num_clients, dtype=object)   \n",
    "    workers = np.ones(numWorkers)\n",
    "                \n",
    "    work = [(i, clientsData[i], clientsDataLabels[i], avg_test_params) for i in range(num_clients)]\n",
    "    \n",
    "    with ThreadPool(len(workers)) as p:\n",
    "        p.map(client_test_metrics, work)\n",
    "    \n",
    "    return metrics_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-bridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(interval_params, ds, test_split, ds_info):\n",
    "    num_clients = ds_info['num_clients']\n",
    "    \n",
    "    (x_train, y_train) = ds\n",
    "    \n",
    "    global x_test\n",
    "    global y_test\n",
    "    (x_test, y_test) = test_split\n",
    "    \n",
    "    Path(experiment_name+\"_res/intervals_res\"+str(num_clients)).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    interval_res = list()\n",
    "    clientsData, clientsDataLabels = prepare_data_for_X_clients(x_train, y_train, ds_info)\n",
    "    \n",
    "    # Proceed to interval search\n",
    "\n",
    "    for i in intervals:\n",
    "        global interval\n",
    "        interval = i\n",
    "\n",
    "        interval_res.append(grid_search_for_X_clients(num_clients, clientsData, clientsDataLabels, intervals_params))\n",
    "\n",
    "    # Extract best interval from search\n",
    "    \n",
    "    best_interval = 0\n",
    "\n",
    "    choice_ratio = np.zeros(num_clients, dtype=object)\n",
    "    choice_ratio.fill((0,0.0))\n",
    "\n",
    "    for index, i_res in enumerate(interval_res):\n",
    "\n",
    "        i_res = [r for r in i_res if not r == 0]\n",
    "\n",
    "        for c, client_scan in enumerate(i_res):\n",
    "            if not client_scan == 0:\n",
    "                curr_acc = client_scan.data.head(1)['val_accuracy'].item()\n",
    "\n",
    "                display(client_scan.data)\n",
    "\n",
    "                if choice_ratio[c][1] <= curr_acc:\n",
    "                    choice_ratio[c] = (intervals[index], curr_acc)      \n",
    "\n",
    "                ## Write dataframes to files\n",
    "                client_scan.data.to_csv(experiment_name+\"_res/intervals_res\"+str(num_clients)+\"/intervals_res_client\"+str(c)+\"_interval_\"+str(intervals[index])+\".csv\")\n",
    "\n",
    "    choice_ratio = np.array([e[0] for e in choice_ratio])\n",
    "\n",
    "    res_ratio = np.zeros(len(intervals), dtype=float)\n",
    "\n",
    "    for i,e in enumerate(intervals):\n",
    "        res_ratio[i] = np.count_nonzero(choice_ratio == e)\n",
    "\n",
    "    res_ratio = res_ratio / num_clients\n",
    "\n",
    "    best_interval = intervals[np.argmax(res_ratio)]\n",
    "    print(\"Ratios:\", res_ratio, \"Best interval:\", best_interval)\n",
    "\n",
    "    ####### Retrain each client to run the test set and get best metrics\n",
    "    intervals_test_res = []\n",
    "    interval = best_interval\n",
    "    intervals_params_test = dict(learn_rate=intervals_params['learn_rate'][0],\n",
    "                            batch_size=intervals_params['batch_size'][0],\n",
    "                            momentum=intervals_params['momentum'][0],\n",
    "                            epochs=intervals_params['epochs'][0], \n",
    "                            act_fn=intervals_params['act_fn'][0])\n",
    "\n",
    "    clientsData, clientsDataLabels = prepare_data_for_X_clients(x_train, y_train, ds_info)\n",
    "\n",
    "    intervals_test_res = test_metrics_for_X_clients(num_clients, clientsData, clientsDataLabels, intervals_params_test)\n",
    "\n",
    "    best_index = 1\n",
    "    intervals_test_res = [r for r in intervals_test_res if not r == 0]\n",
    "    for index, i_res in enumerate(intervals_test_res):\n",
    "\n",
    "        if i_res[1] >= intervals_test_res[best_index][1]:\n",
    "            best_index = index\n",
    "\n",
    "    print(\"Best test accuracy :\", intervals_test_res[best_index][1])\n",
    "    print(\"Loss :\", intervals_test_res[best_index][0])\n",
    "    print(\"Precision :\", intervals_test_res[best_index][2])\n",
    "    print(\"Recall :\", intervals_test_res[best_index][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-cancer",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'svhn_cropped'\n",
    "experiment_name = f\"{dataset_name}_iid\"\n",
    "\n",
    "# Set approximated activation function and best parameters\n",
    "approx_act_fn = [reluApprox]\n",
    "# Set intervals to search, check intervals in the approx function\n",
    "intervals = [3,5,7,10,12]\n",
    "\n",
    "intervals_params = dict(learn_rate=[0.193],\n",
    "                        batch_size=[8],\n",
    "                        momentum=[0.776],\n",
    "                        epochs=[29], \n",
    "                        act_fn=approx_act_fn)\n",
    "\n",
    "ds, test_split, ds_info = load_tf_dataset(dataset_name=dataset_name, decentralized=True, display=True)\n",
    "ds_info['num_clients'] = 10\n",
    "run(intervals_params, ds, test_split, ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-valley",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
