# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/optimizer_v2/adam.py

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from tensorflow.python.framework import ops
from tensorflow.python.keras import backend_config
from tensorflow.python.keras.optimizer_v2 import optimizer_v2
from tensorflow.python.ops import array_ops, control_flow_ops
from tensorflow.python.ops import math_ops
from tensorflow.python.ops import state_ops
import tensorflow as tf
import numpy as np


class AdamApprox(optimizer_v2.OptimizerV2):

    def __init__(self,
                 keep_range=False,
                 learning_rate=0.001,
                 beta_1=0.9,
                 beta_2=0.999,
                 epsilon=1e-7,
                 name='AdamApprox',
                 **kwargs):
        super(AdamApprox, self).__init__(name, **kwargs)
        self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))
        self._set_hyper('decay', self._initial_decay)
        self._set_hyper('beta_1', beta_1)
        self._set_hyper('beta_2', beta_2)
        self.epsilon = epsilon or backend_config.epsilon()

        ######################################################
        self.keep_range = keep_range
        self.precision = 32
        self.coeff_values = {'values': np.array([])}
        ######################################################

    def _create_slots(self, var_list):
        # Create slots for the first and second moments.
        # Separate for-loops to respect the ordering of slot variables from v1.
        for var in var_list:
            self.add_slot(var, 'm')
        for var in var_list:
            self.add_slot(var, 'v')

    def _prepare_local(self, var_device, var_dtype, apply_state):
        super(AdamApprox, self)._prepare_local(var_device, var_dtype, apply_state)

        local_step = math_ops.cast(self.iterations + 1, var_dtype)
        beta_1_t = array_ops.identity(self._get_hyper('beta_1', var_dtype))
        beta_2_t = array_ops.identity(self._get_hyper('beta_2', var_dtype))
        beta_1_power = math_ops.pow(beta_1_t, local_step)
        beta_2_power = math_ops.pow(beta_2_t, local_step)
        lr = (apply_state[(var_device, var_dtype)]['lr_t'] *
              (math_ops.sqrt(1 - beta_2_power) / (1 - beta_1_power)))
        apply_state[(var_device, var_dtype)].update(
            dict(
                lr=lr,
                epsilon=ops.convert_to_tensor_v2_with_dispatch(
                    self.epsilon, var_dtype),
                beta_1_t=beta_1_t,
                beta_1_power=beta_1_power,
                one_minus_beta_1_t=1 - beta_1_t,
                beta_2_t=beta_2_t,
                beta_2_power=beta_2_power,
                one_minus_beta_2_t=1 - beta_2_t))

    def set_weights(self, weights):
        params = self.weights
        # If the weights are generated by Keras V1 optimizer, it includes vhats
        # even without amsgrad, i.e, V1 optimizer has 3x + 1 variables, while V2
        # optimizer has 2x + 1 variables. Filter vhats out for compatibility.
        num_vars = int((len(params) - 1) / 2)
        if len(weights) == 3 * num_vars + 1:
            weights = weights[:len(params)]
        super(AdamApprox, self).set_weights(weights)

    def _resource_apply_dense(self, grad, var, apply_state=None):
        var_device, var_dtype = var.device, var.dtype.base_dtype
        coefficients = ((apply_state or {}).get((var_device, var_dtype))
                        or self._fallback_apply_state(var_device, var_dtype))

        m = self.get_slot(var, 'm')
        v = self.get_slot(var, 'v')

        # THE TRICKSTER
        """
        m_t = beta_1 * m + (1 - beta_1) * grad
        v_t = beta_2 * v + (1 - beta_2) * (grad * grad)
        
        m_t_hat = m_t / (1 - beta_1_power)
        v_t_hat = v_t / (1 - beta_2_power)
        
        var_next = var - lr * 1 / (sqrt(v_t_hat) + epsilon) * m_t_hat
        """
        m_t = state_ops.assign(m, coefficients['beta_1_t'] * m + coefficients['one_minus_beta_1_t'] * grad,
                               use_locking=self._use_locking)
        v_t = state_ops.assign(v, coefficients['beta_2_t'] * v + coefficients['one_minus_beta_2_t'] * (grad * grad),
                               use_locking=self._use_locking)

        m_t_hat = m_t / (1 - coefficients['beta_1_power'])
        v_t_hat = v_t / (1 - coefficients['beta_2_power'])

        v_t_hat_sqrt = math_ops.sqrt(v_t_hat)

        #######################################################################
        coeff_approx = self.get_approx(1 / (v_t_hat_sqrt + coefficients['epsilon']))
        #######################################################################
        var_update = state_ops.assign_sub(
            var, coefficients['lr'] * (m_t_hat * coeff_approx),
            use_locking=self._use_locking
        )

        return control_flow_ops.group(*[var_update, m_t, v_t, m_t_hat, v_t_hat, coeff_approx, v_t_hat_sqrt])

    def _resource_apply_sparse(self, grad, var, indices, apply_state=None):
        return NotImplementedError

    def get_config(self):
        config = super(AdamApprox, self).get_config()
        config.update({
            'learning_rate': self._serialize_hyperparameter('learning_rate'),
            'decay': self._initial_decay,
            'beta_1': self._serialize_hyperparameter('beta_1'),
            'beta_2': self._serialize_hyperparameter('beta_2'),
            'epsilon': self.epsilon
        })
        return config

    def set_precision(self, p):
        self.precision = p

    def get_approx(self, x):
        if self.keep_range:
            self.update_range(x.numpy())

        if self.precision == 'full':
            return x
        else:
            decimal_coeff = 10 ** self.precision

            to_float_64 = tf.dtypes.cast(x, tf.float64)
            to_big = to_float_64 * decimal_coeff
            to_int64 = tf.dtypes.cast(to_big, tf.int64)
            to_float_64_int = tf.dtypes.cast(to_int64, tf.float64)
            precision_cut = to_float_64_int / decimal_coeff

            return tf.dtypes.cast(precision_cut, tf.float32)

    def rescale_range(self):
        np.ndarray.flatten(self.coeff_values['values'])
        self.coeff_values['values'] = np.array([min(self.coeff_values['values']), max(self.coeff_values['values'])])

    def update_range(self, x):
        self.coeff_values['values'] = np.append(self.coeff_values['values'], x)

        if len(self.coeff_values['values']) > 1000000:
            self.rescale_range()

    def get_range(self):
        if self.keep_range:
            self.rescale_range()
        return self.coeff_values['values']
