{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import talos\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import tensorflow.keras.backend as K\n",
    "from federated_library.distributions import qty_skew_distrib, label_skew_distrib, \\\n",
    "    feature_skew_distrib, iid_distrib\n",
    "from federated_library.dataset_loader import load_tf_dataset\n",
    "from federated_library.models_keras import get_model\n",
    "from constants import DATASETS, NR_PARTIES, HP_GRID, SKEWS\n",
    "\n",
    "DEVICE = 'GPU'\n",
    "numWorkers = len(tf.config.list_physical_devices('GPU'))\n",
    "if numWorkers == 0:\n",
    "    DEVICE = 'CPU'\n",
    "    numWorkers = os.cpu_count()\n",
    "\n",
    "print(numWorkers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-bundle",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 3\n",
    "\n",
    "\n",
    "def reluApprox(x):\n",
    "    if degree == 3:\n",
    "        if interval == 3:\n",
    "            return 0.7146 + 1.5000 * K.pow(x/interval, 1) + 0.8793 * K.pow(x/interval, 2)\n",
    "        if interval == 5:\n",
    "            return 0.7865 + 2.5000 * K.pow(x/interval, 1) + 1.88 * K.pow(x/interval, 2)\n",
    "        if interval == 7:\n",
    "            return 0.9003 + 3.5000 * K.pow(x/interval, 1) + 2.9013 * K.pow(x/interval, 2)\n",
    "        if interval == 10:\n",
    "            return 1.1155 + 5 * K.pow(x/interval, 1) + 4.4003 * K.pow(x/interval, 2)\n",
    "        if interval == 12:\n",
    "            return 1.2751 + 6 * K.pow(x/interval, 1) + 5.3803 * K.pow(x/interval, 2)\n",
    "    if degree == 5:\n",
    "        if interval == 7:\n",
    "            return 0.7521 + 3.5000 * K.pow(x/interval, 1) + 4.3825 * K.pow(x/interval, 2) - 1.7281 * K.pow(x/interval, 4)\n",
    "        if interval == 20:\n",
    "            return 1.3127 + 10 * K.pow(x/interval, 1) + 15.7631 * K.pow(x/interval, 2) - 7.6296 * K.pow(x/interval, 4)\n",
    "\n",
    "\n",
    "def sigmoidApprox(x):\n",
    "    if degree == 3:\n",
    "        if interval == 3:\n",
    "            return 0.5 + 0.6997 * K.pow(x/interval, 1) - 0.2649 * K.pow(x/interval, 3)\n",
    "        if interval == 5:\n",
    "            return 0.5 + 0.9917 * K.pow(x/interval, 1) - 0.5592 * K.pow(x/interval, 3)\n",
    "        if interval == 7:\n",
    "            return 0.5 + 1.1511 * K.pow(x/interval, 1) - 0.7517 * K.pow(x/interval, 3)\n",
    "        if interval == 8:\n",
    "            return 0.5 + 1.2010 * K.pow(x/interval, 1) - 0.8156 * K.pow(x/interval, 2)\n",
    "        if interval == 12:\n",
    "            return 0.5 + 1.2384 * K.pow(x/interval, 1) - 0.8647 * K.pow(x/interval, 2)\n",
    "\n",
    "\n",
    "def tanApprox(x):\n",
    "    if degree == 3:\n",
    "        if interval == 1:\n",
    "            return 0.9797 * K.pow(x/interval, 1) - 0.2268 * K.pow(x/interval, 3)\n",
    "        if interval == 2:\n",
    "            return 1.7329 * K.pow(x/interval, 1) - 0.8454 * K.pow(x/interval, 3)\n",
    "        if interval == 3:\n",
    "            return 2.1673 * K.pow(x/interval, 1) - 1.3358 * K.pow(x/interval, 3)\n",
    "        if interval == 5:\n",
    "            return 2.5338 * K.pow(x/interval, 1) - 1.8051 * K.pow(x/interval, 3)\n",
    "        if interval == 7:\n",
    "            return 2.6629 * K.pow(x/interval, 1) - 1.9801 * K.pow(x/interval, 3)\n",
    "        if interval == 12:\n",
    "            return 2.7599 * K.pow(x/interval, 1) - 2.1140 * K.pow(x/interval, 2)\n",
    "    if degree == 12:\n",
    "        print('ooopssss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-smoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(x_train, y_train, x_val, y_val, params):\n",
    "\n",
    "    optimizer = SGD(\n",
    "        learning_rate=params['client_lr'],\n",
    "        momentum=params['client_momentum'],\n",
    "        nesterov=False, name='SGD'\n",
    "    )\n",
    "\n",
    "    model = get_model(params, ds_info)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"mean_squared_error\",\n",
    "        metrics=['accuracy', tf.keras.metrics.Recall(),\n",
    "                 tf.keras.metrics.Precision()],\n",
    "        run_eagerly=False\n",
    "    )\n",
    "\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\",\n",
    "                                                  min_delta=0.01,\n",
    "                                                  patience=10)\n",
    "    history = model.fit(x=x_train,\n",
    "                        y=y_train,\n",
    "                        epochs=params['epochs'],\n",
    "                        batch_size=params['batch_size'],\n",
    "                        validation_split=0.1,\n",
    "                        callbacks=[early_stop],\n",
    "                        verbose=0)\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-board",
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_gridsearch(work):\n",
    "\n",
    "    client_number, clientData, clientDataLabels, param_grid = work\n",
    "\n",
    "    free = np.where(workers == 1)\n",
    "    i = free[0][0]\n",
    "    workers[i] = 0\n",
    "\n",
    "    # Distribute load accross DEVICEs\n",
    "    with tf.device(f\"/{DEVICE}:{i}\"):\n",
    "        print(f\"training on {DEVICE}: {i}\")\n",
    "\n",
    "        scan_results = talos.Scan(x=clientData,\n",
    "                                  y=clientDataLabels,\n",
    "                                  params=param_grid,\n",
    "                                  model=experiment,\n",
    "                                  experiment_name=f\"{experiment_name}_{client_number}\")\n",
    "        scan_res[client_number] = scan_results\n",
    "        print(f\"client running on {DEVICE}: {i} finished\")\n",
    "\n",
    "        workers[i] = 1\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "def grid_search_for_X_clients(numClients, clientsData, clientsDataLabels, param_grid):\n",
    "\n",
    "    global scan_res\n",
    "    global workers\n",
    "\n",
    "    scan_res = np.zeros(numClients, dtype=object)\n",
    "    workers = np.ones(numWorkers)\n",
    "\n",
    "    work = [(i, clientsData[i], clientsDataLabels[i], param_grid)\n",
    "            for i in range(numClients)]\n",
    "\n",
    "    with ThreadPool(len(workers)) as p:\n",
    "        p.map(client_gridsearch, work)\n",
    "\n",
    "    return scan_res\n",
    "\n",
    "\n",
    "def intervals_search_for_X_clients(numClients, clientsData, clientsDataLabels, params, intervals):\n",
    "\n",
    "    global scan_res\n",
    "    global workers\n",
    "\n",
    "    workers = np.ones(numWorkers)\n",
    "\n",
    "    global interval\n",
    "\n",
    "    intervals_res = np.zeros(len(intervals), dtype=object)\n",
    "\n",
    "    for idx, inter in enumerate(intervals):\n",
    "        interval = inter\n",
    "        scan_res = np.zeros(numClients, dtype=object)\n",
    "\n",
    "        work = [(i, clientsData[i], clientsDataLabels[i], params[i])\n",
    "                for i in range(numClients)]\n",
    "\n",
    "        try:\n",
    "            with ThreadPool(len(workers)) as p:\n",
    "                p.map(client_gridsearch, work)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        intervals_res[idx] = scan_res\n",
    "\n",
    "    return intervals_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-making",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run(hyperparams, ds, test_dataset, ds_info, with_intervals=False, display=False):\n",
    "    for c in hyperparams['clients_set']:\n",
    "\n",
    "        ds_info['num_clients'] = c\n",
    "\n",
    "        x_train, y_train = ds\n",
    "\n",
    "        TRAIN_SAMPLES_NUMBER = len(y_train)\n",
    "        print(TRAIN_SAMPLES_NUMBER)\n",
    "\n",
    "        for skew in hyperparams['skews_set']:\n",
    "            BASE_DIR = f\"{dataset_name}_non_iid_res/\"\n",
    "\n",
    "            Path(f\"{BASE_DIR}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            #####################  QUANTITY SKEW BEGIN  #####################\n",
    "            if skew_type == \"qty\":\n",
    "                clientsData, clientsDataLabels = qty_skew_distrib(\n",
    "                    x_train, y_train, ds_info, skew, decentralized=True,\n",
    "                    display=display, is_tf=True\n",
    "                )\n",
    "\n",
    "                # Write in file the percentage of samples each client received\n",
    "                textfile = open(\n",
    "                    f\"{BASE_DIR}{dataset_name}_{skew_type}_skew_{skew}_{c}clients_distribution.txt\", \"w\")\n",
    "                for i, cd in enumerate(clientsData):\n",
    "                    textfile.write(\n",
    "                        f\"client: {i}, samples: {len(cd)} / {TRAIN_SAMPLES_NUMBER}, \"\n",
    "                        f\"percentage: {len(cd)/TRAIN_SAMPLES_NUMBER}\\n\")\n",
    "                textfile.close()\n",
    "            #####################  QUANTITY SKEW END  #####################\n",
    "\n",
    "            #####################  LABEL SKEW BEGIN  ####################\n",
    "            elif skew_type == \"label\":\n",
    "                clientsData, clientsDataLabels = label_skew_distrib(\n",
    "                    x_train, y_train, ds_info, skew, decentralized=True,\n",
    "                    display=display, is_tf=True\n",
    "                )\n",
    "\n",
    "                # Write in file the amount of each class sample each client received\n",
    "                textfile = open(\n",
    "                    f\"{BASE_DIR}{dataset_name}_{skew_type}_skew_{skew}_{c}clients_distribution.txt\", \"w\")\n",
    "\n",
    "                # File header\n",
    "                header = \"client_id\"\n",
    "\n",
    "                for i in range(ds_info['num_classes']):\n",
    "                    header += f\",class_{i}\"\n",
    "                textfile.write(f\"{header}\\n\")\n",
    "\n",
    "                for i, cdl in enumerate(clientsDataLabels):\n",
    "\n",
    "                    line = \"\"\n",
    "                    labelsMap = np.zeros(ds_info['num_classes'], dtype=int)\n",
    "\n",
    "                    for label in cdl:\n",
    "                        labelsMap[np.argmax(label)] += 1\n",
    "\n",
    "                    for _, count in enumerate(labelsMap):\n",
    "                        line += f\",{count}\"\n",
    "\n",
    "                    textfile.write(f\"{i}{line}\\n\")\n",
    "                textfile.close()\n",
    "            #####################  LABEL SKEW END  #####################\n",
    "\n",
    "            #####################  FEATURE SKEW BEGIN  ####################\n",
    "            elif skew_type == \"feature\":\n",
    "                clientsData, clientsDataLabels = feature_skew_distrib(\n",
    "                    x_train, y_train, ds_info, skew, decentralized=True,\n",
    "                    display=display, is_tf=True\n",
    "                )\n",
    "            #####################  FEATURE SKEW END  ####################\n",
    "\n",
    "            #####################  IID BEGIN  ####################\n",
    "            else:\n",
    "                clientsData, clientsDataLabels = iid_distrib(\n",
    "                    x_train, y_train, ds_info, decentralized=True,\n",
    "                    display=display, is_tf=True)\n",
    "            #####################  IID END  ####################\n",
    "\n",
    "            #####################  TRAINING AND TEST BEGIN  #####################\n",
    "            params_grid = dict(act_fn=hyperparams['act_fn'], client_lr=hyperparams['client_lr'],\n",
    "                               client_momentum=hyperparams['client_momentum'],\n",
    "                               batch_size=hyperparams['batch_size'], epochs=hyperparams['epochs'])\n",
    "\n",
    "            res = grid_search_for_X_clients(\n",
    "                c, clientsData, clientsDataLabels, params_grid)\n",
    "            #####################  TRAINING AND TEST END  #####################\n",
    "\n",
    "            #####################  SAVE GRIDSEARCH RESULTS BEGIN  #####################\n",
    "            # Sort results and write to file\n",
    "            sorted_data = []\n",
    "            res = [r for r in res if not r == 0]\n",
    "\n",
    "            for _, scan in enumerate(res):\n",
    "                sorted_data.append(scan.data.sort_values(\n",
    "                    by='val_accuracy', ascending=False))\n",
    "\n",
    "            for i, r in enumerate(sorted_data):\n",
    "                r.to_csv(f\"{BASE_DIR}{skew}_{skew_type}_{c}clts_clt{i}.txt\")\n",
    "            #####################  SAVE GRIDSEARCH RESULTS END  #####################\n",
    "\n",
    "            #####################  INTERVALS SEARCH BEGIN  #####################\n",
    "            if with_intervals:\n",
    "                # Prepare grid for interval search\n",
    "\n",
    "                intervals_grid = []\n",
    "\n",
    "                # Retrieve each best hyperparams for each client\n",
    "                for i in range(c):\n",
    "                    client_i_lr = sorted_data[i].head(\n",
    "                        1).get(['client_lr']).values[0][0]\n",
    "                    client_i_mom = sorted_data[i].head(1).get(\n",
    "                        ['client_momentum']).values[0][0]\n",
    "                    client_i_batch_size = sorted_data[i].head(\n",
    "                        1).get(['batch_size']).values[0][0]\n",
    "\n",
    "                    client_interval_grid = dict(\n",
    "                        act_fn=hyperparams['act_fn_approx'],\n",
    "                        client_lr=[client_i_lr],\n",
    "                        client_momentum=[client_i_mom],\n",
    "                        batch_size=[client_i_batch_size],\n",
    "                        epochs=hyperparams['epochs']\n",
    "                    )\n",
    "                    intervals_grid.append(client_interval_grid)\n",
    "\n",
    "                intervals_res = intervals_search_for_X_clients(\n",
    "                    c, clientsData, clientsDataLabels, intervals_grid, hyperparams['intervals'])\n",
    "\n",
    "                best_interval_per_client = np.zeros(c, dtype=int)\n",
    "\n",
    "                for i in range(c):\n",
    "                    best_client_interval_acc = 0.0\n",
    "                    best_client_interval = 0\n",
    "                    for j in range(len(hyperparams['intervals'])):\n",
    "                        try:\n",
    "                            if (intervals_res[j][i].data.head(1).get(['val_accuracy']).values[0][0] >= best_client_interval_acc):\n",
    "                                best_client_interval = hyperparams['intervals'][j]\n",
    "                                best_client_interval_acc = intervals_res[j][i].data.head(\n",
    "                                    1).get(['val_accuracy']).values[0][0]\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                    best_interval_per_client[i] = best_client_interval\n",
    "\n",
    "                with open(f\"{BASE_DIR}{dataset_name}_{skew_type}_skew_{skew}_{c}clients_intervals.txt\", \"w\") as textfile:\n",
    "                    for i in best_interval_per_client:\n",
    "                        textfile.write(f\"{i}\\n\")\n",
    "            #####################  INTERVALS SEARCH END  #####################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-samoa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dataset_name in DATASETS:\n",
    "    experiment_name = f\"{dataset_name}_non-iid\"\n",
    "    for skew_type in SKEWS.keys():\n",
    "        ds, ds_test, ds_info = load_tf_dataset(\n",
    "            dataset_name=dataset_name, skew_type=skew_type,\n",
    "            decentralized=True, display=False\n",
    "        )\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices(ds_test)\n",
    "        test_dataset = test_dataset.batch(64)\n",
    "\n",
    "        hyperparams = dict(\n",
    "            act_fn=['relu'],\n",
    "            act_fn_approx=[reluApprox],\n",
    "            intervals=HP_GRID[\"interval\"],\n",
    "            client_lr=HP_GRID[\"lr\"],\n",
    "            client_momentum=HP_GRID[\"mom\"],\n",
    "            batch_size=HP_GRID[\"bs\"],\n",
    "            epochs=[30],\n",
    "            clients_set=NR_PARTIES,\n",
    "            skews_set=SKEWS[skew_type]\n",
    "        )\n",
    "\n",
    "        run(hyperparams, ds, test_dataset, ds_info,\n",
    "            with_intervals=True, display=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
